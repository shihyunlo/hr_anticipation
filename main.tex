\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Mathematical model for human-robot motion anticipation in crowd navigation}
\author{Shih-Yun Lo}
\date{September 2017}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

\maketitle

\section{Scenarios}
While deploying a robot in human-populated environments, we observe significant behavioral differences among pedestrians around the robot. This phenomenon is especially obvious when the pedestrian and the robot have paths crossing each other, raising a conflict in their original routes. 

By calculating the crossing timings of both agents, rational/objective decisions over the discrete decision on who to pass first can be made; yet, inaccurate relative velocity estimate of humans in large distances (and other factors, such as low visibility/observability of other agents in crowded environments) commonly causes unreached consensus over avoidance behaviors among pedestrians. The miscommunication results in hesitations and path replanning, which delays the navigation.

While inaccurate relative velocity impairs smoothness in human navigation due to the uncertainty in planning, people usually get a way to reach consensus quickly. However, when encountering a robot crossing, the decision over which way/how to avoid appears largely variant among different people, simply because people do not know how to anticipate robot's motion in such scenarios.

Humans follow several rules to guide their navigation in human-populated 
environments. For example, they follow people with the same short-term 
subgoal; in the U.S., people keep right on the curb and overtake the road from 
the left, which is the opposite in Japan~\cite{zanlungo2012microscopic}. When 
approaching each other, people often make brief eye contact while 
communicating with body language which way they intend to pass each other, and 
then avoid looking at each other afterwards so that there is no chance of 
future confusion.
%In crossing situations, it is commonly observed that people decide one way to avoid and indicate through eye contact for one time. After that, they stick to that path and do not adapt their decision. 
This somehow reckless behavior is supported by the norm that the agent with better visibility to each other is obliged to route adaptation.

However, when considering navigating around a robot, such norms are no longer obeyed, or presumably obeyed, by the robot, simply because of the lack of knowledge of the robot's capability. It is unclear to a person, if the robot sees the dynamic agent; even if he/she thinks it sees the pedestrian, the anticipation of how and when the robot will avoid is not commonly established. We therefore observe people frequently yielding to the robot during crossing, staying away from a large distance even when overtaking the route. From the above, we can conclude that people have high uncertainty about the robot's motion, and it is largely affecting their behaviors around the robot, especially in crossing situations where certain social norms are presumed.

To address the mismatched human navigation behaviors between that around a human and around a robot, we propose a model on human's anticipation in a joint scenario, where both agents' decisions affect each other's policies. 

\section{Human anticipation model}
One common feedback of people crossing a robot is, "I don't know how to predict where the robot is going". We also have seen people trying to pass in front of the robot, but hesitated after approaching the robot without receiving its imminent yielding response. We not only consider what the other agent is going to do, but also what the agent will \textbf{react} to my action: if people assume that the robot will not yield in collision-dangerous situations, or will avoid in an unpredictable fashion, they tend to take relatively conservative actions: they tend to yield even when he/she is supposed to lead the crossing situation, or they watch the robot constantly and keep a large safety distance during the crossing period.  

To model the above behavior, we propose the following formulation of human's decision with anticipation of other agent's policy:
\begin{equation}
a^H_t \sim \pi^H (\hat{x}^H_t, \hat{x}^R_t, b(\pi^R (\hat{x}^R_t, \hat{x}^H_t,b(\pi^H) ))) ,
\end{equation}
where the state vectors $x^H, x^R$ represent the positions and velocities of the human and the robot. $a^H$ is the action, in the form of acceleration, of the human. The policy of human $\pi^H$ is modeled to take in his/her noisy estimate of the states $\hat{x}^H$ and $\hat{x}^R$ due to imperfect sensing capability, and the \textbf{belief} of the other agent's policy. We call this the first step anticipation. 

While one can take in the robot's belief of human's policy in the human's 
belief of the robot's policy to continue the second step anticipation for 
equilibrium study, we hypothesize that this is computationally too expensive 
for human's online processing to deal with dynamic environments, therefore we 
only consider it to the first step, to approximate the decision making process 
to, "what would the robot do when I do this", as:
\begin{equation}
a^H_t \sim \pi^H (\hat{x}^H_t, \hat{x}^R_t, b(\pi^R (\hat{x}^R_t, \hat{x}^H_t,a^H_t ))).
\end{equation}
While there may be different policy assumptions in human's navigation behavior for collision avoidance, due to the nature of resource conflict, we consider human's decision making formulation, as a minimax process, where the person is going to select the action (or policy) that outputs the best value consider the worst case scenarios:
\begin{equation}
a^{*H}_t = argmax_{a^H \in A^H} V^H(\hat{x}^H_t, a^H ,\hat{x}^R,\pi^R_{min}),
\end{equation}
where,
\begin{equation}
    \pi^R_{min} = argmin_{\pi^R \in \Pi^R} V^H(\hat{x}^H_t,a^H, \hat{x}^R, \pi^R),
\end{equation}
is the worst reactive robot behavior among potential robot policies $\Pi^R$, given the current human action $a^H$. This worst case scenario is assess by the human, so evaluated by the human's value function $V^H$. 

Here the policy of the robot $\pi^R$ can be considered as part of the dynamics of the system, or a parameter in the environment that affects the value function $V^H$. 

Since different people may hypothesize robot behavior very differently, the set of potential robot policies, $\Pi^R$ may vary largely among people. For example, with the assumption of "the robot may not be able to react safely to avoid me", the pedestrian avoids far from the robot. The opposite happens to people who trust the robot to not collide with a human in any case. Potentially, people adapt their behaviors once they observe and update the robot's policy. 

The other factor that affects people's reactive policy is their strategy over their hypothesized robot reactions, which is assumed here as a minimax function. However, if, for example, the human supposes the robot as a collaborative agent, less conservative behavior is expected compared to that predicted by the minimax function.

Other than the hypotheses people pose over the robot's behavior, people may also have different value functions $V^H$ over their own actions, such as how much they penalize deviating from their original route, accelerations...etc.

A stochastic policy can take in, for example, the softmax representation commonly seen to represent human behaviors:
\begin{equation}
p(a^{*H}) \propto exp( V^H(\hat{x}^H_t, a^H ,\hat{x}^R,\pi^R_{min})).
\end{equation}

\section{Social Force Model}



% 2. single agent decision making model to incorporate hri:
% 1. applying SFM for prediction: from a technical perspective
% 3. challenge: apply SFM for stochastic decisions, and the stochacity in SADMM





\bibliographystyle{plain}
\bibliography{references}
\end{document}
